{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Photometric Classification To Fit Results\n",
    "\n",
    "This notebook applies the photometric classification method from González-Gaitán et al. 2014 to SDSS, DES, and CSP light-curve fits. Results are then used to analyze the properties of peculiar supernovae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.table import Table\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import optimize\n",
    "from sklearn.utils import resample\n",
    "from sndata.csp import dr1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We begin by reading in spectroscopic classificaions for supernovae observed by CSP DR1. We join these classifications with our own classification parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "photo_class = Table.read('../results/csp_dr3_simple_fit_class.ecsv').to_pandas(index='obj_id')\n",
    "\n",
    "spec_class = dr1.load_table(1)['SN', 'Type'].to_pandas(index='SN')\n",
    "spec_class.fillna('Unknown', inplace=True)\n",
    "\n",
    "classification = photo_class.join(spec_class)\n",
    "classification['Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us enough information to replicate the classification figure from Gonzalez-Gaitan+ 14. Note that we draw dashed lines intersecting at (0, 0) for visual reference only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification(class_data):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    for sn_type in set(class_data['Type']):\n",
    "        i = class_data['Type'] == sn_type\n",
    "        plt.scatter(class_data['x'][i], \n",
    "                    class_data['y'][i], \n",
    "                    label=sn_type)\n",
    "\n",
    "    plt.axhline(0, linestyle='--', color='black', alpha=.6)\n",
    "    plt.axvline(0, linestyle='--', color='black', alpha=.6)\n",
    "    plt.xlabel(r'$\\chi^2_{blue}(Ia) - \\chi^2_{blue}(91bg)$', fontsize=14)\n",
    "    plt.ylabel(r'$\\chi^2_{red}(Ia) - \\chi^2_{red}(91bg)$', fontsize=14)\n",
    "    plt.xlim(-6000, 6000)\n",
    "    plt.ylim(-6000, 6000)\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification(classification)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize FOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a figure of merit (FOM) value as an optimization parameter for training our classification. The FOM is defined as:\n",
    "\n",
    "$$FOM = \\frac{N_{true}}{N_{tot}} * \\frac{N_{true}}{N_{true} + N_{false}}$$\n",
    "\n",
    "where $N_{true}$ is the number of correctly identified objects of a given type (e.g. 91bg-like objects), $N_{tot}$ is the total input number of that type and $N_{false}$ is the number of objects. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write a few functions to determine the optimal chi-squared boundaries based on the FOM. We include functions for two types of classification boundaries. The first calculates the FOM using a verticle and horizontal boundary. The second uses a single boundary at an angle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rect_fom(dataframe, blue_cutoff, red_cutoff):\n",
    "    \"\"\"Calculate the rectangular figure of merrit for a set of classifications\n",
    "    \n",
    "    args:\n",
    "        dataframe   (float): \n",
    "        blue_cutoff (float): \n",
    "        red_cutoff  (float): \n",
    "        \n",
    "    Returns:\n",
    "        The figure of merit value\n",
    "    \"\"\"\n",
    "\n",
    "    is_91bg_true = dataframe['Type'] == '91bg'\n",
    "    is_91bg_classified = (\n",
    "            (dataframe['x'] > blue_cutoff)\n",
    "            & (dataframe['y'] > red_cutoff)\n",
    "    )\n",
    "\n",
    "    num_tot = sum(is_91bg_true)  # How to handle multiple types?\n",
    "    num_true = sum(is_91bg_true == is_91bg_classified)\n",
    "    num_false = len(dataframe) - num_true\n",
    "    return (num_true / num_tot) * (num_true / (num_true + num_false))\n",
    "\n",
    "\n",
    "def calc_diagonal_fom(dataframe, m, b):\n",
    "    \"\"\"Calculate the rectangular figure of merrit for a set of classifications\n",
    "    \n",
    "    args:\n",
    "        dataframe   (float): \n",
    "        blue_cutoff (float): \n",
    "        red_cutoff  (float): \n",
    "        \n",
    "    Returns:\n",
    "        The figure of merit value\n",
    "    \"\"\"\n",
    "\n",
    "    is_91bg_true = dataframe['Type'] == '91bg'\n",
    "    is_91bg_predict = m * dataframe['x'] + b\n",
    "    is_91bg_classified = dataframe['y'] >= is_91bg_predict\n",
    "\n",
    "    num_tot = sum(is_91bg_true)  # How to handle multiple types?\n",
    "    num_true = sum(is_91bg_true == is_91bg_classified)\n",
    "    num_false = len(dataframe) - num_true\n",
    "    return (num_true / num_tot) * (num_true / (num_true + num_false))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Rectangular FOM:')\n",
    "initial_fom = calc_rect_fom(classification, 0, 0)\n",
    "print(f'FOM at (0, 0): {initial_fom}\\n')\n",
    "\n",
    "inverse_fom = lambda args: 1 / calc_rect_fom(classification, *args)\n",
    "rect_result = optimize.minimize(inverse_fom, [.1, 0])\n",
    "rect_result['fun'] = 1 / rect_result['fun']\n",
    "print('Optimization results:')\n",
    "print(rect_result)\n",
    "\n",
    "print('\\n\\nAngled FOM:')\n",
    "initial_fom = calc_diagonal_fom(classification, -1, 0)\n",
    "print(f'FOM at (-1, 0): {initial_fom}\\n')\n",
    "\n",
    "inverse_fom = lambda args: 1 / calc_diagonal_fom(classification, *args)\n",
    "angl_result = optimize.minimize(inverse_fom, [-1, 0])\n",
    "angl_result['fun'] = 1 / angl_result['fun']\n",
    "print('Optimization results:')\n",
    "print(angl_result)\n",
    "\n",
    "print('\\n\\nAngled FOM at 45 degrees:')\n",
    "initial_fom = calc_diagonal_fom(classification, -1, 0)\n",
    "print(f'FOM at (-1, 0): {initial_fom}\\n')\n",
    "\n",
    "inverse_fom = lambda b: 1 / calc_diagonal_fom(classification, -1, b)\n",
    "angl_45_result = optimize.minimize(inverse_fom, 0)\n",
    "angl_45_result['fun'] = 1 / angl_45_result['fun']\n",
    "print('Optimization results:')\n",
    "print(angl_45_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "a = max(np.abs(classification['x']))\n",
    "line_points = np.array([-a, a])\n",
    "\n",
    "for sn_type in set(classification['Type']):\n",
    "    i = classification['Type'] == sn_type\n",
    "    plt.scatter(classification['x'].loc[i], \n",
    "                classification['y'].loc[i], \n",
    "                label=sn_type)\n",
    "\n",
    "plt.axvline(rect_result.x[0], linestyle='--', color='black', alpha=.6)\n",
    "plt.axhline(rect_result.x[1], linestyle='--', color='black', alpha=.6)\n",
    "\n",
    "# Rectangular FOM\n",
    "plt.axvline(rect_result.x[0], linestyle='--', color='red', alpha=.6)\n",
    "plt.axhline(rect_result.x[1], linestyle='--', color='red', alpha=.6,\n",
    "            label=f'FOM = {rect_result.fun:.4f}')\n",
    "\n",
    "# Angled FOM\n",
    "angl_line = angl_result.x[0] * line_points + angl_result.x[1]\n",
    "plt.plot(line_points, angl_line, linestyle='-', color='orange', alpha=.6,\n",
    "        label=f'FOM = {angl_result.fun:.4f}')\n",
    "\n",
    "# Angled FOM at 45 degrees\n",
    "angl_45_line = -line_points + angl_result.x[0]\n",
    "plt.plot(line_points, angl_45_line, linestyle='-.', color='green', alpha=.6,\n",
    "         label=f'FOM = {angl_45_result.fun:.4f}')\n",
    "\n",
    "plt.xlabel(r'$\\chi^2_{blue}(Ia) - \\chi^2_{blue}(91bg)$', fontsize=14)\n",
    "plt.ylabel(r'$\\chi^2_{red}(Ia) - \\chi^2_{red}(91bg)$', fontsize=14)\n",
    "plt.xlim(-6000, 6000)\n",
    "plt.ylim(-6000, 6000)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a figure of merit optimization, we can bootstrap our data to determine our final classification parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure bootstrap\n",
    "n_iterations = 1000\n",
    "n_size = int(len(classification) * 0.50)\n",
    "\n",
    "# run bootstrap\n",
    "fom_values = []\n",
    "classification_params = []\n",
    "for i in range(n_iterations):\n",
    "    # prepare train and test sets\n",
    "    sample_data = resample(classification, n_samples=n_size)\n",
    "    inverse_fom = lambda args: 1 / calc_rect_fom(classification, *args)\n",
    "    result = optimize.minimize(inverse_fom, [0, 0])\n",
    "\n",
    "    fom_values.append(1 / result.fun)\n",
    "    classification_params.append(result.x)\n",
    "\n",
    "classification_params = np.array(classification_params).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_confidence_intervals(alpha, stats):\n",
    "    p = ((1 - alpha) / 2) * 100\n",
    "    lower = max(0, np.percentile(stats, p))\n",
    "\n",
    "    p = (alpha + ((1 - alpha) / 2)) * 100\n",
    "    upper = min(1.0, np.percentile(stats, p))\n",
    "\n",
    "    return lower, upper\n",
    "\n",
    "\n",
    "alpha=0.95\n",
    "confidence = 0.95\n",
    "average_fom = np.average(fom_values)\n",
    "fom_interval = calc_confidence_intervals(confidence, fom_values)\n",
    "\n",
    "print(f'Average FOM: {average_fom}')\n",
    "print(f'{alpha * 100:.1f} confidence interval: [{fom_interval[0] * 100:.1f} '\n",
    "      f', {fom_interval[1] * 100:.1f}]')\n",
    "\n",
    "average_params = np.average(classification_params, axis=1)\n",
    "blue_param_interval = calc_confidence_intervals(confidence, classification_params[0])\n",
    "red_param_interval = np.average(classification_params[1])\n",
    "\n",
    "print(f'Average classification params: {average_params}')\n",
    "print(f'{alpha * 100:.1f} confidence interval for blue '\n",
    "      f'param: [{blue_param_interval[0] * 100:.1f} '\n",
    "      f', {blue_param_interval[1] * 100:.1f}]')\n",
    "\n",
    "print(f'{alpha * 100:.1f} confidence interval for red '\n",
    "      f'param: [{red_param_interval[0] * 100:.1f} '\n",
    "      f', {red_param_interval[1] * 100:.1f}]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sdss_chi_diff['blue'], sdss_chi_diff['red'], label='SDSS')\n",
    "plt.scatter(des_chi_diff['blue'], des_chi_diff['red'], label='DES')\n",
    "plt.scatter(csp_chi_diff['blue'], csp_chi_diff['red'], label='CSP')\n",
    "\n",
    "plt.axvline(0, linestyle='--', color='black', alpha=.6)\n",
    "plt.axhline(0, linestyle='--', color='black', alpha=.6)\n",
    "plt.axvline(average_params[0], linestyle='--', color='red', alpha=.6)\n",
    "plt.axhline(average_params[1], linestyle='--', color='red', alpha=.6,\n",
    "            label=f'FOM = {1 / result.fun}')\n",
    "\n",
    "plt.xlabel(r'$\\chi^2_{blue}(Ia) - \\chi^2_{blue}(91bg)$', fontsize=14)\n",
    "plt.ylabel(r'$\\chi^2_{red}(Ia) - \\chi^2_{red}(91bg)$', fontsize=14)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sdss_class] *",
   "language": "python",
   "name": "conda-env-sdss_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
